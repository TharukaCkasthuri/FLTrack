{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4103d89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import collections\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from models import ShallowNN\n",
    "from utils import load_file, get_all_possible_pairs\n",
    "from evals import evaluate, pairwise_euclidean_distance , influence, layer_importance, layer_importance_bias, layerwise_full_accumulated_proximity\n",
    "from evals import euclidean_distance, pairwise_euclidean_distance, accumulated_proximity\n",
    "\n",
    "features = 197\n",
    "batch_size = 64\n",
    "loss_fn = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "240fb247",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_ids = [\"0_0\",\"0_1\",\"0_2\",\"0_3\",\"0_4\",\"0_5\",\"1_0\",\"1_1\",\"1_2\",\"1_3\",\"1_4\",\"1_5\",\"2_0\",\"2_1\",\"2_2\",\"2_3\",\"2_4\",\"2_5\",\"3_0\",\"3_1\",\"3_2\",\"3_3\",\"3_4\",\"3_5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce3e8ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = torch.load(\"checkpt/isolated/batch64_client_0_0.pth\")\n",
    "critarians = [item for item in dummy]\n",
    "state_dicts = {\n",
    "        key: torch.load(\"checkpt/isolated/batch64_client_\" + str(key) + \".pth\")\n",
    "        for key in client_ids\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc90d67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layerwise_proximity(x:collections.OrderedDict,y:collections.OrderedDict, critarian:str , distance_matrix):\n",
    "    if critarian.split(\".\")[-1] == \"bias\":\n",
    "        proximity = accumulated_proximity(x[critarian].view(1, -1),y[critarian].view(1, -1),distance_matrix)\n",
    "    else:\n",
    "        proximity = accumulated_proximity(x[critarian],y[critarian],distance_matrix)\n",
    "    \n",
    "    return proximity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffd4adb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layerwise_full_accumulated_proximity(\n",
    "    clients: list, criterian: str, distance_matrix) -> tuple:\n",
    "    \"\"\"\n",
    "    Calculate the layer-wise full accumulated proximity between all clients.\n",
    "\n",
    "    Parameters:\n",
    "    -------------\n",
    "    clients: list; list of clients\n",
    "    criterian: str; criterian to be evaluated, basically the layer name.\n",
    "    distance_matrix: Callable; distance matrix\n",
    "\n",
    "    Returns:\n",
    "    -------------\n",
    "    total_weight_proximity: float; total weight proximity\n",
    "    total_bias_proximity: float; total bias proximity\n",
    "    \"\"\"\n",
    "    state_dicts = {\n",
    "        key: torch.load(\"checkpt/isolated/batch64_client_\" + str(key) + \".pth\")\n",
    "        for key in clients\n",
    "    }\n",
    "\n",
    "    #possible_pairs = get_all_possible_pairs(clients)\n",
    "\n",
    "    total_proximity = 0.0\n",
    "\n",
    "    for l in clients:\n",
    "        for i in clients:\n",
    "            prox = layerwise_proximity(state_dicts[l],state_dicts[i],criterian, distance_matrix)\n",
    "            total_proximity += prox\n",
    "\n",
    "    return total_proximity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6ac2825",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_prox_dict = {}\n",
    "for item in critarians:\n",
    "    full_prox_dict[item] = layerwise_full_accumulated_proximity(client_ids, item ,euclidean_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa22d6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eccentricities = {}\n",
    "for c in critarians:\n",
    "    client_ecc = {}\n",
    "    for client in client_ids:\n",
    "        client_matrix = torch.load(\"checkpt/isolated/batch64_client_\" + str(client) + \".pth\")\n",
    "        acc_proximity = 0.0\n",
    "        for key in state_dicts:\n",
    "            distance = layerwise_proximity(client_matrix,state_dicts[key],c, euclidean_distance)\n",
    "            acc_proximity += distance\n",
    "        eccentricity = 2*acc_proximity/full_prox_dict[c]\n",
    "        client_ecc[client] = eccentricity.item()\n",
    "    eccentricities[c] = client_ecc\n",
    "    #print(client,acc_proximity, 2*acc_proximity/layer1_fullweight_prox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abb2cdbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer_1.weight</th>\n",
       "      <th>layer_1.bias</th>\n",
       "      <th>layer_2.weight</th>\n",
       "      <th>layer_2.bias</th>\n",
       "      <th>layer_3.weight</th>\n",
       "      <th>layer_3.bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0_0</th>\n",
       "      <td>0.083633</td>\n",
       "      <td>0.079671</td>\n",
       "      <td>0.081799</td>\n",
       "      <td>0.083995</td>\n",
       "      <td>0.084671</td>\n",
       "      <td>0.059368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_1</th>\n",
       "      <td>0.084282</td>\n",
       "      <td>0.078892</td>\n",
       "      <td>0.081980</td>\n",
       "      <td>0.082882</td>\n",
       "      <td>0.085825</td>\n",
       "      <td>0.065470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_2</th>\n",
       "      <td>0.080969</td>\n",
       "      <td>0.076763</td>\n",
       "      <td>0.081529</td>\n",
       "      <td>0.079244</td>\n",
       "      <td>0.080579</td>\n",
       "      <td>0.095041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_3</th>\n",
       "      <td>0.081178</td>\n",
       "      <td>0.083053</td>\n",
       "      <td>0.082016</td>\n",
       "      <td>0.076486</td>\n",
       "      <td>0.080884</td>\n",
       "      <td>0.100829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_4</th>\n",
       "      <td>0.079436</td>\n",
       "      <td>0.073525</td>\n",
       "      <td>0.081037</td>\n",
       "      <td>0.081065</td>\n",
       "      <td>0.079129</td>\n",
       "      <td>0.065578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     layer_1.weight  layer_1.bias  layer_2.weight  layer_2.bias  \\\n",
       "0_0        0.083633      0.079671        0.081799      0.083995   \n",
       "0_1        0.084282      0.078892        0.081980      0.082882   \n",
       "0_2        0.080969      0.076763        0.081529      0.079244   \n",
       "0_3        0.081178      0.083053        0.082016      0.076486   \n",
       "0_4        0.079436      0.073525        0.081037      0.081065   \n",
       "\n",
       "     layer_3.weight  layer_3.bias  \n",
       "0_0        0.084671      0.059368  \n",
       "0_1        0.085825      0.065470  \n",
       "0_2        0.080579      0.095041  \n",
       "0_3        0.080884      0.100829  \n",
       "0_4        0.079129      0.065578  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrp_eccetricities = pd.DataFrame.from_dict(eccentricities)\n",
    "lrp_eccetricities.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8f6f82c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer_1.weight</th>\n",
       "      <th>layer_1.bias</th>\n",
       "      <th>layer_2.weight</th>\n",
       "      <th>layer_2.bias</th>\n",
       "      <th>layer_3.weight</th>\n",
       "      <th>layer_3.bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0_0</th>\n",
       "      <td>84.786891</td>\n",
       "      <td>0.735403</td>\n",
       "      <td>13.686570</td>\n",
       "      <td>0.387663</td>\n",
       "      <td>0.362551</td>\n",
       "      <td>0.040923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_1</th>\n",
       "      <td>80.435967</td>\n",
       "      <td>0.867103</td>\n",
       "      <td>17.431104</td>\n",
       "      <td>0.598819</td>\n",
       "      <td>0.581139</td>\n",
       "      <td>0.085869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_2</th>\n",
       "      <td>83.258224</td>\n",
       "      <td>0.594982</td>\n",
       "      <td>15.057051</td>\n",
       "      <td>0.490581</td>\n",
       "      <td>0.530299</td>\n",
       "      <td>0.068863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_3</th>\n",
       "      <td>85.642939</td>\n",
       "      <td>0.614633</td>\n",
       "      <td>12.952055</td>\n",
       "      <td>0.352219</td>\n",
       "      <td>0.395308</td>\n",
       "      <td>0.042847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_4</th>\n",
       "      <td>83.370004</td>\n",
       "      <td>0.593724</td>\n",
       "      <td>15.024360</td>\n",
       "      <td>0.460909</td>\n",
       "      <td>0.485463</td>\n",
       "      <td>0.065541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     layer_1.weight  layer_1.bias  layer_2.weight  layer_2.bias  \\\n",
       "0_0       84.786891      0.735403       13.686570      0.387663   \n",
       "0_1       80.435967      0.867103       17.431104      0.598819   \n",
       "0_2       83.258224      0.594982       15.057051      0.490581   \n",
       "0_3       85.642939      0.614633       12.952055      0.352219   \n",
       "0_4       83.370004      0.593724       15.024360      0.460909   \n",
       "\n",
       "     layer_3.weight  layer_3.bias  \n",
       "0_0        0.362551      0.040923  \n",
       "0_1        0.581139      0.085869  \n",
       "0_2        0.530299      0.068863  \n",
       "0_3        0.395308      0.042847  \n",
       "0_4        0.485463      0.065541  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_importance_scores = {}\n",
    "  \n",
    "for client in client_ids:\n",
    "    iso_model = ShallowNN(features)\n",
    "    iso_model.load_state_dict(torch.load(\"checkpt/isolated/batch64_client_\" + str(client) + \".pth\"))\n",
    "    validation_data = torch.load(\"trainpt/\" + str(client) + \".pt\")\n",
    "    validation_data_loader = DataLoader(validation_data, batch_size, shuffle=True)\n",
    "    iso_layer_importance = layer_importance_bias(iso_model, loss_fn, validation_data_loader)\n",
    "    \n",
    "    layer_importance_scores[client] = iso_layer_importance\n",
    "layer_importance = pd.DataFrame.from_dict(layer_importance_scores).T\n",
    "layer_importance.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "569dbd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_1.weight 2.0\n",
      "layer_im 81.56235189417863\n",
      "layer_1.bias 2.0\n",
      "layer_im 0.757794185471357\n",
      "layer_2.weight 2.0\n",
      "layer_im 16.60970214029278\n",
      "layer_2.bias 2.0\n",
      "layer_im 0.5456082225863877\n",
      "layer_3.weight 2.0\n",
      "layer_im 0.4648452783645767\n",
      "layer_3.bias 2.0\n",
      "layer_im 0.0596982791062732\n"
     ]
    }
   ],
   "source": [
    "for item in critarians:\n",
    "    print(item, round(lrp_eccetricities[item].sum(),4))\n",
    "    print(\"layer_im\",layer_importance[item].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1d2516a",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1_weight_ls = 81.60\n",
    "layer_1_bias_ls = 0.76\n",
    "layer_2_weight_ls = 16.57\n",
    "layer_2_bias_ls = 0.54\n",
    "layer_3_weight_ls = 0.47\n",
    "layer_3_bias_ls = 0.06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "615279f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "averages = [] \n",
    "weighted_avg = []\n",
    "for index, client in lrp_eccetricities.iterrows():\n",
    "    avg = (client['layer_1.weight'] + client['layer_1.bias'] + client['layer_2.weight'] + client['layer_2.bias'] + client['layer_3.weight'] + client['layer_3.bias'])/6\n",
    "    weighted = (client['layer_1.weight']*layer_1_weight_ls + client['layer_1.bias']*layer_1_bias_ls + \n",
    "                client['layer_2.weight']*layer_2_weight_ls + client['layer_2.bias']*layer_2_bias_ls + \n",
    "                client['layer_3.weight']*layer_3_weight_ls + client['layer_3.bias']*layer_3_bias_ls)/(100)\n",
    "    averages.append(round(avg,4))\n",
    "    weighted_avg.append(round(weighted,4))\n",
    "lrp_eccetricities[\"average\"] = averages\n",
    "lrp_eccetricities[\"weighted_avg\"] = weighted_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a42e683",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrp_eccetricities.to_csv(\"insights/eccentricity_with_lrp.csv\" , index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b813b906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer_1.weight</th>\n",
       "      <th>layer_1.bias</th>\n",
       "      <th>layer_2.weight</th>\n",
       "      <th>layer_2.bias</th>\n",
       "      <th>layer_3.weight</th>\n",
       "      <th>layer_3.bias</th>\n",
       "      <th>average</th>\n",
       "      <th>weighted_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0_0</th>\n",
       "      <td>0.083633</td>\n",
       "      <td>0.079671</td>\n",
       "      <td>0.081799</td>\n",
       "      <td>0.083995</td>\n",
       "      <td>0.084671</td>\n",
       "      <td>0.059368</td>\n",
       "      <td>0.0789</td>\n",
       "      <td>0.0833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_1</th>\n",
       "      <td>0.084282</td>\n",
       "      <td>0.078892</td>\n",
       "      <td>0.081980</td>\n",
       "      <td>0.082882</td>\n",
       "      <td>0.085825</td>\n",
       "      <td>0.065470</td>\n",
       "      <td>0.0799</td>\n",
       "      <td>0.0838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_2</th>\n",
       "      <td>0.080969</td>\n",
       "      <td>0.076763</td>\n",
       "      <td>0.081529</td>\n",
       "      <td>0.079244</td>\n",
       "      <td>0.080579</td>\n",
       "      <td>0.095041</td>\n",
       "      <td>0.0824</td>\n",
       "      <td>0.0810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_3</th>\n",
       "      <td>0.081178</td>\n",
       "      <td>0.083053</td>\n",
       "      <td>0.082016</td>\n",
       "      <td>0.076486</td>\n",
       "      <td>0.080884</td>\n",
       "      <td>0.100829</td>\n",
       "      <td>0.0841</td>\n",
       "      <td>0.0813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_4</th>\n",
       "      <td>0.079436</td>\n",
       "      <td>0.073525</td>\n",
       "      <td>0.081037</td>\n",
       "      <td>0.081065</td>\n",
       "      <td>0.079129</td>\n",
       "      <td>0.065578</td>\n",
       "      <td>0.0766</td>\n",
       "      <td>0.0797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_5</th>\n",
       "      <td>0.084520</td>\n",
       "      <td>0.088152</td>\n",
       "      <td>0.082386</td>\n",
       "      <td>0.100017</td>\n",
       "      <td>0.082506</td>\n",
       "      <td>0.086066</td>\n",
       "      <td>0.0873</td>\n",
       "      <td>0.0843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_0</th>\n",
       "      <td>0.084846</td>\n",
       "      <td>0.085740</td>\n",
       "      <td>0.082735</td>\n",
       "      <td>0.087511</td>\n",
       "      <td>0.085409</td>\n",
       "      <td>0.061738</td>\n",
       "      <td>0.0813</td>\n",
       "      <td>0.0845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_1</th>\n",
       "      <td>0.084173</td>\n",
       "      <td>0.078355</td>\n",
       "      <td>0.083445</td>\n",
       "      <td>0.085079</td>\n",
       "      <td>0.084149</td>\n",
       "      <td>0.071878</td>\n",
       "      <td>0.0812</td>\n",
       "      <td>0.0840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_2</th>\n",
       "      <td>0.085419</td>\n",
       "      <td>0.092366</td>\n",
       "      <td>0.080363</td>\n",
       "      <td>0.086349</td>\n",
       "      <td>0.080833</td>\n",
       "      <td>0.065815</td>\n",
       "      <td>0.0819</td>\n",
       "      <td>0.0846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_3</th>\n",
       "      <td>0.083477</td>\n",
       "      <td>0.082536</td>\n",
       "      <td>0.083019</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.083659</td>\n",
       "      <td>0.092411</td>\n",
       "      <td>0.0832</td>\n",
       "      <td>0.0833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_4</th>\n",
       "      <td>0.085537</td>\n",
       "      <td>0.094475</td>\n",
       "      <td>0.084056</td>\n",
       "      <td>0.086027</td>\n",
       "      <td>0.089133</td>\n",
       "      <td>0.063261</td>\n",
       "      <td>0.0837</td>\n",
       "      <td>0.0854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_5</th>\n",
       "      <td>0.088242</td>\n",
       "      <td>0.084153</td>\n",
       "      <td>0.084134</td>\n",
       "      <td>0.098077</td>\n",
       "      <td>0.086365</td>\n",
       "      <td>0.060269</td>\n",
       "      <td>0.0835</td>\n",
       "      <td>0.0876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_0</th>\n",
       "      <td>0.079899</td>\n",
       "      <td>0.081845</td>\n",
       "      <td>0.082332</td>\n",
       "      <td>0.071294</td>\n",
       "      <td>0.084776</td>\n",
       "      <td>0.131734</td>\n",
       "      <td>0.0886</td>\n",
       "      <td>0.0803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_1</th>\n",
       "      <td>0.080550</td>\n",
       "      <td>0.079811</td>\n",
       "      <td>0.083135</td>\n",
       "      <td>0.078047</td>\n",
       "      <td>0.081048</td>\n",
       "      <td>0.059555</td>\n",
       "      <td>0.0770</td>\n",
       "      <td>0.0809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_2</th>\n",
       "      <td>0.080747</td>\n",
       "      <td>0.083239</td>\n",
       "      <td>0.083558</td>\n",
       "      <td>0.076409</td>\n",
       "      <td>0.083572</td>\n",
       "      <td>0.103826</td>\n",
       "      <td>0.0852</td>\n",
       "      <td>0.0812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_3</th>\n",
       "      <td>0.080836</td>\n",
       "      <td>0.079501</td>\n",
       "      <td>0.083559</td>\n",
       "      <td>0.077704</td>\n",
       "      <td>0.083639</td>\n",
       "      <td>0.092929</td>\n",
       "      <td>0.0830</td>\n",
       "      <td>0.0813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_4</th>\n",
       "      <td>0.080294</td>\n",
       "      <td>0.084050</td>\n",
       "      <td>0.083019</td>\n",
       "      <td>0.077767</td>\n",
       "      <td>0.084861</td>\n",
       "      <td>0.084317</td>\n",
       "      <td>0.0824</td>\n",
       "      <td>0.0808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_5</th>\n",
       "      <td>0.080384</td>\n",
       "      <td>0.075470</td>\n",
       "      <td>0.083431</td>\n",
       "      <td>0.081045</td>\n",
       "      <td>0.083468</td>\n",
       "      <td>0.064917</td>\n",
       "      <td>0.0781</td>\n",
       "      <td>0.0809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_0</th>\n",
       "      <td>0.085528</td>\n",
       "      <td>0.083886</td>\n",
       "      <td>0.085074</td>\n",
       "      <td>0.090548</td>\n",
       "      <td>0.079465</td>\n",
       "      <td>0.168781</td>\n",
       "      <td>0.0989</td>\n",
       "      <td>0.0855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_1</th>\n",
       "      <td>0.085450</td>\n",
       "      <td>0.081223</td>\n",
       "      <td>0.086229</td>\n",
       "      <td>0.083355</td>\n",
       "      <td>0.085060</td>\n",
       "      <td>0.078975</td>\n",
       "      <td>0.0834</td>\n",
       "      <td>0.0855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_2</th>\n",
       "      <td>0.084381</td>\n",
       "      <td>0.088261</td>\n",
       "      <td>0.084221</td>\n",
       "      <td>0.090246</td>\n",
       "      <td>0.083877</td>\n",
       "      <td>0.080813</td>\n",
       "      <td>0.0853</td>\n",
       "      <td>0.0844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_3</th>\n",
       "      <td>0.085205</td>\n",
       "      <td>0.083270</td>\n",
       "      <td>0.086079</td>\n",
       "      <td>0.086329</td>\n",
       "      <td>0.079800</td>\n",
       "      <td>0.059368</td>\n",
       "      <td>0.0800</td>\n",
       "      <td>0.0853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_4</th>\n",
       "      <td>0.085356</td>\n",
       "      <td>0.089124</td>\n",
       "      <td>0.084031</td>\n",
       "      <td>0.091984</td>\n",
       "      <td>0.082535</td>\n",
       "      <td>0.059657</td>\n",
       "      <td>0.0821</td>\n",
       "      <td>0.0852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_5</th>\n",
       "      <td>0.085654</td>\n",
       "      <td>0.092639</td>\n",
       "      <td>0.086833</td>\n",
       "      <td>0.074322</td>\n",
       "      <td>0.084754</td>\n",
       "      <td>0.127403</td>\n",
       "      <td>0.0919</td>\n",
       "      <td>0.0859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     layer_1.weight  layer_1.bias  layer_2.weight  layer_2.bias  \\\n",
       "0_0        0.083633      0.079671        0.081799      0.083995   \n",
       "0_1        0.084282      0.078892        0.081980      0.082882   \n",
       "0_2        0.080969      0.076763        0.081529      0.079244   \n",
       "0_3        0.081178      0.083053        0.082016      0.076486   \n",
       "0_4        0.079436      0.073525        0.081037      0.081065   \n",
       "0_5        0.084520      0.088152        0.082386      0.100017   \n",
       "1_0        0.084846      0.085740        0.082735      0.087511   \n",
       "1_1        0.084173      0.078355        0.083445      0.085079   \n",
       "1_2        0.085419      0.092366        0.080363      0.086349   \n",
       "1_3        0.083477      0.082536        0.083019      0.074219   \n",
       "1_4        0.085537      0.094475        0.084056      0.086027   \n",
       "1_5        0.088242      0.084153        0.084134      0.098077   \n",
       "2_0        0.079899      0.081845        0.082332      0.071294   \n",
       "2_1        0.080550      0.079811        0.083135      0.078047   \n",
       "2_2        0.080747      0.083239        0.083558      0.076409   \n",
       "2_3        0.080836      0.079501        0.083559      0.077704   \n",
       "2_4        0.080294      0.084050        0.083019      0.077767   \n",
       "2_5        0.080384      0.075470        0.083431      0.081045   \n",
       "3_0        0.085528      0.083886        0.085074      0.090548   \n",
       "3_1        0.085450      0.081223        0.086229      0.083355   \n",
       "3_2        0.084381      0.088261        0.084221      0.090246   \n",
       "3_3        0.085205      0.083270        0.086079      0.086329   \n",
       "3_4        0.085356      0.089124        0.084031      0.091984   \n",
       "3_5        0.085654      0.092639        0.086833      0.074322   \n",
       "\n",
       "     layer_3.weight  layer_3.bias  average  weighted_avg  \n",
       "0_0        0.084671      0.059368   0.0789        0.0833  \n",
       "0_1        0.085825      0.065470   0.0799        0.0838  \n",
       "0_2        0.080579      0.095041   0.0824        0.0810  \n",
       "0_3        0.080884      0.100829   0.0841        0.0813  \n",
       "0_4        0.079129      0.065578   0.0766        0.0797  \n",
       "0_5        0.082506      0.086066   0.0873        0.0843  \n",
       "1_0        0.085409      0.061738   0.0813        0.0845  \n",
       "1_1        0.084149      0.071878   0.0812        0.0840  \n",
       "1_2        0.080833      0.065815   0.0819        0.0846  \n",
       "1_3        0.083659      0.092411   0.0832        0.0833  \n",
       "1_4        0.089133      0.063261   0.0837        0.0854  \n",
       "1_5        0.086365      0.060269   0.0835        0.0876  \n",
       "2_0        0.084776      0.131734   0.0886        0.0803  \n",
       "2_1        0.081048      0.059555   0.0770        0.0809  \n",
       "2_2        0.083572      0.103826   0.0852        0.0812  \n",
       "2_3        0.083639      0.092929   0.0830        0.0813  \n",
       "2_4        0.084861      0.084317   0.0824        0.0808  \n",
       "2_5        0.083468      0.064917   0.0781        0.0809  \n",
       "3_0        0.079465      0.168781   0.0989        0.0855  \n",
       "3_1        0.085060      0.078975   0.0834        0.0855  \n",
       "3_2        0.083877      0.080813   0.0853        0.0844  \n",
       "3_3        0.079800      0.059368   0.0800        0.0853  \n",
       "3_4        0.082535      0.059657   0.0821        0.0852  \n",
       "3_5        0.084754      0.127403   0.0919        0.0859  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrp_eccetricities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95628b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
