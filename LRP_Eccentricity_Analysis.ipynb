{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4103d89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import collections\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from models import ShallowNN\n",
    "from utils import load_file, get_all_possible_pairs\n",
    "from evals import evaluate, pairwise_euclidean_distance , influence, layer_importance, layer_importance_bias, layerwise_full_accumulated_proximity\n",
    "from evals import euclidean_distance, pairwise_euclidean_distance, accumulated_proximity\n",
    "\n",
    "features = 197\n",
    "batch_size = 64\n",
    "loss_fn = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "240fb247",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_ids = [\"0_0\",\"0_1\",\"0_2\",\"0_3\",\"0_4\",\"0_5\",\"1_0\",\"1_1\",\"1_2\",\"1_3\",\"1_4\",\"1_5\",\"2_0\",\"2_1\",\"2_2\",\"2_3\",\"2_4\",\"2_5\",\"3_0\",\"3_1\",\"3_2\",\"3_3\",\"3_4\",\"3_5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce3e8ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = torch.load(\"checkpt/isolated/batch64_client_0_0.pth\")\n",
    "critarians = [item for item in dummy]\n",
    "state_dicts = {\n",
    "        key: torch.load(\"checkpt/isolated/batch64_client_\" + str(key) + \".pth\")\n",
    "        for key in client_ids\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc90d67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layerwise_proximity(x:collections.OrderedDict,y:collections.OrderedDict, critarian:str , distance_matrix):\n",
    "    if critarian.split(\".\")[-1] == \"bias\":\n",
    "        proximity = accumulated_proximity(x[critarian].view(1, -1),y[critarian].view(1, -1),distance_matrix)\n",
    "    else:\n",
    "        proximity = accumulated_proximity(x[critarian],y[critarian],distance_matrix)\n",
    "    \n",
    "    return proximity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffd4adb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layerwise_full_accumulated_proximity(\n",
    "    clients: list, criterian: str, distance_matrix) -> tuple:\n",
    "    \"\"\"\n",
    "    Calculate the layer-wise full accumulated proximity between all clients.\n",
    "\n",
    "    Parameters:\n",
    "    -------------\n",
    "    clients: list; list of clients\n",
    "    criterian: str; criterian to be evaluated, basically the layer name.\n",
    "    distance_matrix: Callable; distance matrix\n",
    "\n",
    "    Returns:\n",
    "    -------------\n",
    "    total_weight_proximity: float; total weight proximity\n",
    "    total_bias_proximity: float; total bias proximity\n",
    "    \"\"\"\n",
    "    state_dicts = {\n",
    "        key: torch.load(\"checkpt/isolated/batch64_client_\" + str(key) + \".pth\")\n",
    "        for key in clients\n",
    "    }\n",
    "\n",
    "    #possible_pairs = get_all_possible_pairs(clients)\n",
    "\n",
    "    total_proximity = 0.0\n",
    "\n",
    "    for l in clients:\n",
    "        for i in clients:\n",
    "            prox = layerwise_proximity(state_dicts[l],state_dicts[i],criterian, distance_matrix)\n",
    "            total_proximity += prox\n",
    "\n",
    "    return total_proximity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ac2825",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_prox_dict = {}\n",
    "for item in critarians:\n",
    "    full_prox_dict[item] = layerwise_full_accumulated_proximity(client_ids, item ,euclidean_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa22d6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eccentricities = {}\n",
    "for c in critarians:\n",
    "    client_ecc = {}\n",
    "    for client in client_ids:\n",
    "        client_matrix = torch.load(\"checkpt/isolated/batch64_client_\" + str(client) + \".pth\")\n",
    "        acc_proximity = 0.0\n",
    "        for key in state_dicts:\n",
    "            distance = layerwise_proximity(client_matrix,state_dicts[key],c, euclidean_distance)\n",
    "            acc_proximity += distance\n",
    "        eccentricity = 2*acc_proximity/full_prox_dict[c]\n",
    "        client_ecc[client] = eccentricity.item()\n",
    "    eccentricities[c] = client_ecc\n",
    "    #print(client,acc_proximity, 2*acc_proximity/layer1_fullweight_prox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb2cdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrp_eccetricities = pd.DataFrame.from_dict(eccentricities)\n",
    "lrp_eccetricities.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f6f82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_importance_scores = {}\n",
    "  \n",
    "for client in client_ids:\n",
    "    iso_model = ShallowNN(features)\n",
    "    iso_model.load_state_dict(torch.load(\"checkpt/isolated/batch64_client_\" + str(client) + \".pth\"))\n",
    "    validation_data = torch.load(\"trainpt/\" + str(client) + \".pt\")\n",
    "    validation_data_loader = DataLoader(validation_data, batch_size, shuffle=True)\n",
    "    iso_layer_importance = layer_importance_bias(iso_model, loss_fn, validation_data_loader)\n",
    "    \n",
    "    layer_importance_scores[client] = iso_layer_importance\n",
    "layer_importance = pd.DataFrame.from_dict(layer_importance_scores).T\n",
    "layer_importance.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569dbd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in critarians:\n",
    "    print(item, round(lrp_eccetricities[item].sum(),4))\n",
    "    print(\"layer_im\",layer_importance[item].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d2516a",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1_weight_ls = 81.60\n",
    "layer_1_bias_ls = 0.76\n",
    "layer_2_weight_ls = 16.57\n",
    "layer_2_bias_ls = 0.54\n",
    "layer_3_weight_ls = 0.47\n",
    "layer_3_bias_ls = 0.06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615279f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "averages = [] \n",
    "weighted_avg = []\n",
    "for index, client in lrp_eccetricities.iterrows():\n",
    "    avg = (client['layer_1.weight'] + client['layer_1.bias'] + client['layer_2.weight'] + client['layer_2.bias'] + client['layer_3.weight'] + client['layer_3.bias'])/6\n",
    "    weighted = (client['layer_1.weight']*layer_1_weight_ls + client['layer_1.bias']*layer_1_bias_ls + \n",
    "                client['layer_2.weight']*layer_2_weight_ls + client['layer_2.bias']*layer_2_bias_ls + \n",
    "                client['layer_3.weight']*layer_3_weight_ls + client['layer_3.bias']*layer_3_bias_ls)/(100)\n",
    "    averages.append(round(avg,4))\n",
    "    weighted_avg.append(round(weighted,4))\n",
    "lrp_eccetricities[\"average\"] = averages\n",
    "lrp_eccetricities[\"weighted_avg\"] = weighted_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a42e683",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrp_eccetricities#.to_csv(\"insights/eccentricity_with_lrp.csv\" , index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
